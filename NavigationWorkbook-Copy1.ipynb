{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3ac8604",
   "metadata": {},
   "source": [
    "## Import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0edf2280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "# Standard Modules\n",
    "import os\n",
    "import ast\n",
    "import ujson\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque #namedtuple\n",
    "\n",
    "# Deep Learning Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plotting Modules\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Define if GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Activate Auto-completer\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc8c08a",
   "metadata": {},
   "source": [
    "## Prepare gaming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8353bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "try: env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "except: env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "action_size = brain.vector_action_space_size\n",
    "state_size = len(env_info.vector_observations[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d9eca3-93b4-464c-9792-8b0914aa506d",
   "metadata": {},
   "source": [
    "## Define the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93820678",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''Defines the Neural Network\n",
    "    \n",
    "    state_size (int):      The size if the sate, will be used for the size of the input layer\n",
    "    action_size (int):     The number of available actions, will be used for the output size\n",
    "    hidden_layers ([int]): List of hidden layers to include. \n",
    "                           The amount of numbers will be the amount of hidden layers, \n",
    "                           and the number will represent the amount of perceptons to include \n",
    "    drop_out (float):      The percentage of perceptions to drop out (0-1).\n",
    "    play (Bool):           True returns a Tensor arrray of actions, False returns integer action\n",
    "    ''' \n",
    "    def __init__(self,state_size, action_size, hidden_layers, drop_out):\n",
    "        super().__init__()                \n",
    "        # list of Networks for Sequential\n",
    "        layerlist = []\n",
    "                \n",
    "        if len(hidden_layers) > 0:\n",
    "            # First Layer\n",
    "            layerlist.append(nn.Linear(state_size, hidden_layers[0]))\n",
    "            layerlist.append(nn.ReLU(inplace=True))\n",
    "            \n",
    "            # Hidden Layer\n",
    "            last_layer = hidden_layers[0]\n",
    "            for layer in hidden_layers:\n",
    "                layerlist.append(nn.Linear(last_layer, layer))\n",
    "                layerlist.append(nn.Dropout(p=drop_out))\n",
    "                layerlist.append(nn.ReLU(inplace=True))  \n",
    "                last_layer = layer\n",
    "            # Final Layer\n",
    "            layerlist.append(nn.Linear(last_layer, action_size))\n",
    "        else:\n",
    "            layerlist.append(nn.Linear(state_size, action_size))\n",
    "            \n",
    "        self.sequence = nn.Sequential(*layerlist)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.Tensor(x)\n",
    "        x = self.sequence(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53cfde-e896-41d0-bf00-c996c8af4704",
   "metadata": {},
   "source": [
    "# Define the agent to navigate the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce51fc9e-629b-47fa-852f-a606cc3ed555",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    '''Defines the agent that interacts and learns with the environment'''\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, tau, gamma, learning_rate, memory_size, replay_size, update_frequency, hidden_layers, drop_out):\n",
    "        \n",
    "        # Preset variables for learning\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.memory_size = memory_size\n",
    "        self.replay_size = replay_size\n",
    "        self.update_frequency = update_frequency\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.losses = []\n",
    "\n",
    "        # instantiate the local and target network\n",
    "        torch.manual_seed(seed)  \n",
    "        self.local_net = Net(state_size, action_size, hidden_layers, drop_out).to(device)\n",
    "        self.target_net = Net(state_size, action_size, hidden_layers, drop_out).to(device)\n",
    "        self.optimizer = optim.Adam(self.local_net.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # instantiate memory replay\n",
    "        self.memory = MemoryReply(self.memory_size, self.replay_size, seed) #action_size goes first\n",
    "        \n",
    "        # Initialize time step (for updating every update_frequency steps)\n",
    "        self.freq_update = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Add an experice and kick out an old one\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Update learn frequency counter\n",
    "        self.freq_update += 1\n",
    "        \n",
    "        # Learn after defined learn update frequency and once enough samples are collected\n",
    "        if (self.freq_update % self.update_frequency) == 0:\n",
    "            if len(self.memory) > self.replay_size:\n",
    "                experiences = self.memory.sample()\n",
    "                loss = self.learn(experiences)\n",
    "                self.losses.append(loss.item())\n",
    "            else:\n",
    "                self.losses.append(np.nan)\n",
    "\n",
    "    def act(self, state, eps=0.):\n",
    "        '''Action givern current state. Returns an integer if in non train mode, otherwise returns a tensor of possible actions.\n",
    "        \n",
    "        state      : Current state\n",
    "        eps (float): Epsilon, preset to zero for non train mode\n",
    "        '''\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            self.local_net.eval()\n",
    "            with torch.no_grad():\n",
    "                action_values = self.local_net(state)\n",
    "            self.local_net.train()\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return np.random.choice(action_size,p=[0.6,0.06,0.17,0.17])\n",
    "\n",
    "    def learn(self, experiences):\n",
    "        '''Update value parameters using given batch of experience tuples.\n",
    "\n",
    "        experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "        gamma (float): discount factor\n",
    "\n",
    "        Soft update: θ_target = τ * θ_local + (1 - τ) * θ_target\n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets = self.target_net(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (self.gamma * Q_targets * (1 - dones))\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.local_net(states).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Soft update target network\n",
    "        for target_param, local_param in zip(self.target_net.parameters(), self.local_net.parameters()):\n",
    "            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)\n",
    "        \n",
    "        # Return the loss to plot\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872df233-1daf-4e61-abdf-314341085fde",
   "metadata": {},
   "source": [
    "# Define the Experience Replay"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2da3462a-138f-47a7-b22a-5b237c0b0d48",
   "metadata": {},
   "source": [
    "class ReplayBuffer:\n",
    "    '''Defines the memory replay of stored samples.\n",
    "    \n",
    "    action_size (int) : dimension of each action\n",
    "    memory_size (int) : maximum size of buffer\n",
    "    replay_size (int) : size of each training batch\n",
    "    seed        (int) : random seed\n",
    "    '''\n",
    "    def __init__(self, action_size, memory_size, replay_size, seed):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)  \n",
    "        self.replay_size = replay_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        '''Store samples to memory\n",
    "        state      ([float]) : The current state space of the givern envirnment\n",
    "        action         (int) : The stochastic or predicted action for the current state space\n",
    "        reward         (int) : The reward recieved for that action\n",
    "        next_state ([float]) : The next state space of the givern envirnment after an action has been taken\n",
    "        done          (bool) : Whether the envirnment has been completed or not\n",
    "        '''\n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        '''Sample experiences from memory.'''\n",
    "        experiences = random.sample(self.memory, k=self.replay_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "rb = ReplayBuffer(4,200000,3,123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94f7ec50-bdee-4438-90bb-12fb669dd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryReply:\n",
    "    '''Defines the memory replay of stored samples.\n",
    "    \n",
    "    action_size (int) : dimension of each action\n",
    "    memory_size (int) : maximum size of buffer\n",
    "    replay_size (int) : size of each training batch\n",
    "    seed        (int) : random seed\n",
    "    '''\n",
    "    def __init__(self, memory_size, replay_size, seed): #action_size\n",
    "        #self.action_size = action_size\n",
    "        self.memory = deque(maxlen=memory_size)  \n",
    "        self.replay_size = replay_size\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        '''Store samples to memory\n",
    "        state      ([float]) : The current state space of the givern envirnment\n",
    "        action         (int) : The stochastic or predicted action for the current state space\n",
    "        reward         (int) : The reward recieved for that action\n",
    "        next_state ([float]) : The next state space of the givern envirnment after an action has been taken\n",
    "        done          (bool) : Whether the envirnment has been completed or not\n",
    "        '''\n",
    "        self.memory.append({\"state\":state, \"action\":action, \"reward\":reward, \"next_state\":next_state, \"done\":done})\n",
    "    \n",
    "    def sample(self):\n",
    "        '''Sample experiences from memory.'''\n",
    "        experiences = random.sample(self.memory, k=self.replay_size)\n",
    "\n",
    "        states = torch.FloatTensor(np.array([e['state'] for e in experiences])).to(device)\n",
    "        actions = torch.LongTensor(np.array([e['action'] for e in experiences])).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(np.array([e['reward'] for e in experiences])).unsqueeze(1).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([e['next_state'] for e in experiences])).to(device)\n",
    "        dones = torch.FloatTensor(np.array([float(e['done']) for e in experiences])).unsqueeze(1).to(device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9608f74-5f05-4b37-b455-96da4cc8576b",
   "metadata": {},
   "source": [
    "mr = MemoryReply(10,3,123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca7bb1-cbc5-4fd5-b163-8072ddb08f59",
   "metadata": {},
   "source": [
    "mr.add([1,2,3],2,-1,[3,2,1],False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d223f64f-efe2-402a-b39e-ef462f0d5f52",
   "metadata": {},
   "source": [
    "mr.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0429e43-a040-4975-a090-436d6f102216",
   "metadata": {},
   "source": [
    "# Define the plotting funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53187c2-a441-4e02-9024-9d65100a086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(scores, scores_window_plot, loss, save_plot = False):\n",
    "    '''Plots the score and loss\n",
    "    \n",
    "    scores             ([float]) : List of all the final scores\n",
    "    scores_window_plot ([float]) : List of the final scores averaged window\n",
    "    loss_window        ([float]) : List of losses averaged per episode\n",
    "    save_plot             (Bool) : Saves the plot to a .png file\n",
    "    '''\n",
    "    # Create Subplots\n",
    "    width = len(scores)*0.015\n",
    "    if width < 5  : width = 5\n",
    "    if width > 15 : width = 15\n",
    "    fig, axs = plt.subplots(2,figsize=(width,8),sharex=True)\n",
    "    \n",
    "    # Score Plot\n",
    "    axs[0].plot(scores,label='Scores per Episode')\n",
    "    axs[0].plot(scores_window_plot,color='red',label='Moving Average (100 Episode Moving Average)')\n",
    "    axs[0].set_title('Metrics')\n",
    "    axs[0].set(ylabel='Score')\n",
    "    axs[0].grid()\n",
    "    axs[0].legend(loc=\"upper left\")\n",
    "    \n",
    "    # Loss plot\n",
    "    axs[1].plot(loss)\n",
    "    axs[1].set(xlabel='Epochs', ylabel='Loss (MSE)')\n",
    "    axs[1].grid()\n",
    "    \n",
    "    # Display\n",
    "    fig.tight_layout()\n",
    "    if save_plot == True:\n",
    "        plt.savefig('Images/AverageScoreLoss.png')\n",
    "    plt.show()\n",
    "    return \n",
    "\n",
    "def plot_trained_score(scores,save_plot = False):\n",
    "    '''Plots the score and loss\n",
    "    \n",
    "    scores ([float]) : List of all the final scores\n",
    "    save_plot (Bool) : Saves the plot to a .png file\n",
    "    '''    \n",
    "    # Score Plot\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(scores,label='Score per Game')\n",
    "    plt.plot([np.mean(score_100_games) for i in range(len(score_100_games))],color='red',label='Average Score')\n",
    "    plt.title('Scores of Games played on a Traind Network')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Game Number')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "    # save plot\n",
    "    if save_plot == True:\n",
    "        plt.savefig('Images/TainedNetworkScores.png')\n",
    "        \n",
    "    # Display    \n",
    "    plt.show()\n",
    "    return \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb3f22-69f7-4dda-86ed-7272a0bbcbbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define the Deep Q-Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9bd9b3-6c45-4dab-a1f1-e61412368b72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dqn(episodes = 1800, starting_eps = 1.0, eps_end = 0.01, eps_decay = 0.995, target_score = 15, print_every = 50, seed = 123,\n",
    "        tau = 0.001, gamma = 0.99, learning_rate = 0.0005, memory_size = 200000, replay_size = 200, update_frequency = 5, \n",
    "        parameter_tunning=False, hidden_layers=[148,148], drop_out= 0.25, train_mode=True, save_plot=False, return_score=False):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    \n",
    "    episodes         (int) : Number of training iterations\n",
    "    starting_eps   (float) : Epsilon starting value\n",
    "    eps_end        (float) : Epsilon final value\n",
    "    eps_decay      (float) : Epsilon decay factor\n",
    "    target_score     (int) : Target score to solve enviroment for early stopping\n",
    "    print_every      (int) : How often to print val\n",
    "    state_size       (int) : Size if the sate\n",
    "    action_size      (int) : Size of the executable actions\n",
    "    seed             (int) : Random seed\n",
    "    tau            (float) : Soft update parameter\n",
    "    gamma          (float) : Discount factor\n",
    "    learning_rate  (float) : Learning rate\n",
    "    memory_size      (int) : Replay memory size\n",
    "    replay_size      (int) : Replay Sample Size\n",
    "    update_frequency (int) : Soft update frequency\n",
    "    hidden_layers  ([int]) : List of hidden layers to include. \n",
    "                             The amount of numbers will be the amount of hidden layers, \n",
    "                             and the number will represent the amount of perceptons to include \n",
    "    drop_out       (float) : The percentage of perceptions to drop out (0-1).\n",
    "    train_mode      (Bool) : If set to True, the network will train on the set parameters and save the model.\n",
    "    save_plot       (Bool) : If set to True, the plot will be saved, only if train_mode is set to True.\n",
    "    return_score    (Bool) : Reurns the final rewrd when train_mode = False, to analyse performance over several games from a trained network\n",
    "    \"\"\"  \n",
    "    # initiate the agent\n",
    "    agent = Agent(state_size, action_size, seed, tau, gamma, learning_rate, \n",
    "                  memory_size, replay_size, update_frequency, hidden_layers, drop_out)\n",
    "    \n",
    "    # preset variables for score\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)  \n",
    "    scores_window_plot = []\n",
    "    \n",
    "    # preset variables for loss\n",
    "    loss = []\n",
    "    loss_window = deque(maxlen=100)\n",
    "    eps = starting_eps\n",
    "    \n",
    "    # reset episodes to 1 if not in train mode\n",
    "    if train_mode == False: \n",
    "        episodes =1\n",
    "    \n",
    "    # iterate through defined episodes\n",
    "    for i in range(1, episodes+1):\n",
    "        env_info = env.reset(train_mode=train_mode)[brain_name] \n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        environmentSolved = False\n",
    "        if train_mode == True:\n",
    "\n",
    "            while True:\n",
    "                # Predict action\n",
    "                action = agent.act(state, eps)\n",
    "                \n",
    "                # Update environment information\n",
    "                env_info = env.step(action)[brain_name]       \n",
    "                next_state = env_info.vector_observations[0]  \n",
    "                reward = env_info.rewards[0]                   \n",
    "                done = env_info.local_done[0]    \n",
    "                \n",
    "                # Take next step\n",
    "                agent.step(state, action, reward, next_state, done)\n",
    "                \n",
    "                # Update next state and score\n",
    "                state = next_state\n",
    "                score += reward\n",
    "                if done:\n",
    "                    # record the games loss\n",
    "                    loss.append(np.mean(np.mean([x for x in agent.losses if np.isnan(x) == False])))\n",
    "                    loss_window.append(np.mean(np.mean([x for x in agent.losses if np.isnan(x) == False])))\n",
    "                    \n",
    "                    # reset the loss\n",
    "                    agent.losses = []\n",
    "                    \n",
    "                    # record the games score\n",
    "                    scores.append(score)              \n",
    "                    scores_window.append(score)\n",
    "                    scores_window_plot.append(np.mean(scores_window))\n",
    "                    break \n",
    "                    \n",
    "            # Decay Epsilon\n",
    "            eps = max(eps_end, eps_decay*eps)\n",
    "            \n",
    "            s1,s2 = '            ','              '\n",
    "            if parameter_tunning == False:\n",
    "                print(f'\\rEpisode{s1}{i:>4}{s2}\\tAverage Score: {np.mean(scores_window):.2f}\\t Mean Loss (MSE): {np.mean(loss_window):.6f}', end=\"\")\n",
    "                if i % print_every == 0:\n",
    "                    print(f'\\rEpisode{s1}{i:>4}{s2}\\tAverage Score: {np.mean(scores_window):.2f}\\t Mean Loss (MSE): {np.mean(loss_window):.6f}')\n",
    "            else:\n",
    "                print(f'\\rEpisode{s1}{i:>4}{s2}\\tAverage Score: {np.mean(scores_window):.2f}\\t Mean Loss (MSE): {np.mean(loss_window):.6f}', end=\"\")\n",
    "            \n",
    "            # check if the environment has been solved\n",
    "            if np.mean(scores_window)>=target_score:\n",
    "                print(f'\\rEnvironment Solved {i-100:>4} episodes!\\tAverage Score: {np.mean(scores_window):.2f}\\t Mean Loss (MSE): {np.mean(loss_window):.6f}')\n",
    "                \n",
    "                # set the solved for parameter tuning\n",
    "                environmentSolved = True\n",
    "                \n",
    "                # Save final model\n",
    "                if parameter_tunning == False:    \n",
    "                    torch.save(agent.local_net.state_dict(), 'TrainedModel/banana_model.pth')\n",
    "                    ujson.dump({\"hidden_layers\":hidden_layers,\"drop_out\":drop_out},open('TrainedModel/network_settings.json','w'))\n",
    "                break\n",
    "                    \n",
    "        if train_mode == False:\n",
    "            assert os.path.exists('TrainedModel/banana_model.pth') and os.path.exists('TrainedModel/network_settings.json'),'No trained network detected, please train a network first!'\n",
    "            network_settings = ujson.load(open('TrainedModel/network_settings.json','r'))\n",
    "            trained_net = Net(state_size,action_size,hidden_layers = network_settings['hidden_layers'],drop_out = network_settings['drop_out'])\n",
    "            state_dict = torch.load('TrainedModel/banana_model.pth')\n",
    "            trained_net.load_state_dict(state_dict)\n",
    "            trained_net.eval()\n",
    "            while True:\n",
    "                # Predict action\n",
    "                action = int(torch.argmax(trained_net(state)))\n",
    "                \n",
    "                # Update environment information\n",
    "                env_info = env.step(action)[brain_name]       \n",
    "                next_state = env_info.vector_observations[0]  \n",
    "                reward = env_info.rewards[0]                   \n",
    "                done = env_info.local_done[0]    \n",
    "                \n",
    "                # Update next state and score\n",
    "                score += reward    \n",
    "                state = next_state    \n",
    "                print(f'\\rReward = {score}',end='')\n",
    "                if done == True:\n",
    "                    print(f'\\rReward = {score} GAME COMPLETE',end='')\n",
    "                    break\n",
    "                                \n",
    "    if (parameter_tunning == False) and (train_mode==False):\n",
    "        if return_score == True:\n",
    "            return score\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    elif (parameter_tunning == False) and (train_mode==True):\n",
    "        # plot the score and loss\n",
    "        plot(scores=scores,scores_window_plot=scores_window_plot,loss=loss,save_plot=save_plot)\n",
    "        return\n",
    "\n",
    "    elif (parameter_tunning == True) and (train_mode==True):\n",
    "        # Save and return output from parameter tunning\n",
    "        if environmentSolved == True:\n",
    "            solved_episodes = i-100\n",
    "        else:\n",
    "            solved_episodes = np.nan\n",
    "        # Dictionary of parameters and results for the hyperparameter tunning.\n",
    "        paramenters_used = {\"tau\":tau, \"gamma\":gamma, \"learning_rate\":learning_rate, \"memory_size\":memory_size,\n",
    "                            \"replay_size\":replay_size, \"update_frequency\":update_frequency,\n",
    "                            \"starting_eps\":starting_eps,\"eps_end\":eps_end,\"eps_decay\":eps_decay,\n",
    "                            \"target_score\":target_score,\"solved_episodes\":solved_episodes,\"scores\":scores,\n",
    "                            \"scores_window_plot\":scores_window_plot,\"loss\":loss,\"episodes\":episodes,\n",
    "                            \"hidden_layers\":hidden_layers,\"drop_out\":drop_out,\"environmentSolved\":environmentSolved}\n",
    "\n",
    "        return paramenters_used\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b9991-ce15-49d3-b248-ece03c2a9501",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hyper Paramter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f8be2f6-a0f4-4947-8580-e3c12a85b1e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameter_tunning = False\n",
    "tau = [0.001,0.0008]\n",
    "gamma = [0.99,0.995]\n",
    "learning_rate = [0.001,0.0005]\n",
    "memory_size = [100000,150000,200000]\n",
    "replay_size = [100,150,200]\n",
    "update_frequency = [5,10]\n",
    "hidden_layers=[[37,37],[74,74],[100,100],[148,148]]\n",
    "drop_out=[0,0.15,0.25,0.35,0.5]\n",
    "\n",
    "parameter_list = [tau,gamma,learning_rate,memory_size,replay_size,update_frequency,hidden_layers,drop_out]\n",
    "parameter_combinations = list(itertools.product(*parameter_list))\n",
    "random.shuffle(parameter_combinations)\n",
    "\n",
    "if os.path.exists('ParameterResults.csv'): \n",
    "    TuningDf_loaded = pd.read_csv('ParameterResults.csv')\n",
    "    \n",
    "    # Remove all parameetrs already tested\n",
    "    for row in TuningDf_loaded[[\"tau\",\"gamma\",\"learning_rate\",\"memory_size\",\"replay_size\",\n",
    "                            \"update_frequency\",\"hidden_layers\",\"drop_out\"]].iterrows():\n",
    "        t = row[1]['tau']\n",
    "        g = row[1]['gamma']\n",
    "        lr = row[1]['learning_rate']\n",
    "        ms = row[1]['memory_size']\n",
    "        rs = row[1]['replay_size']\n",
    "        uf = row[1]['update_frequency']\n",
    "        hl = ast.literal_eval(row[1]['hidden_layers'])\n",
    "        do = row[1]['drop_out']\n",
    "        if (t,g,lr,ms,rs,uf,hl,do) in parameter_combinations:\n",
    "            parameter_combinations.remove((t,g,lr,ms,rs,uf,hl,do))\n",
    "else:\n",
    "    # Create empty DataFrame is no file created for parameter tunning to concatenate\n",
    "    TuningDf_loaded = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "if parameter_tunning == True:\n",
    "    # set emoty list for results from tunning\n",
    "    list_parameter_results = []\n",
    "    \n",
    "    # iterate over unique parameters\n",
    "    for t,g,lr,ms,rs,uf,hl,do in parameter_combinations:\n",
    "        parameters = dqn(episodes=800,tau = t, gamma = g, learning_rate = lr, memory_size = ms, replay_size = rs, update_frequency = uf, \n",
    "                         hidden_layers=hl, drop_out= do, target_score = 15 , parameter_tunning = parameter_tunning)\n",
    "\n",
    "        list_parameter_results.append(parameters)\n",
    "        \n",
    "        # concatenate results and save as csv\n",
    "        TuningDf = pd.concat([TuningDf_loaded,pd.DataFrame(list_parameter_results)])\n",
    "        TuningDf = TuningDf.sort_values('solved_episodes').reset_index(drop=True)\n",
    "        TuningDf.to_csv('ParameterResults.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19a31a-b26e-44c3-94ff-860e814d3116",
   "metadata": {},
   "source": [
    "# Picking the best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc257fa-9c0b-49b0-8e83-e58617dba4b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tau</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>memory_size</th>\n",
       "      <th>replay_size</th>\n",
       "      <th>update_frequency</th>\n",
       "      <th>hidden_layers</th>\n",
       "      <th>drop_out</th>\n",
       "      <th>solved_episodes</th>\n",
       "      <th>environmentSolved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>200000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>318.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>150000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>324.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>200000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>325.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>150000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>330.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>150000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.15</td>\n",
       "      <td>336.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>200000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>343.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>200000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.15</td>\n",
       "      <td>346.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>150000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[148, 148]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>347.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>100000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[74, 74]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>351.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.990</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>200000</td>\n",
       "      <td>200</td>\n",
       "      <td>5</td>\n",
       "      <td>[100, 100]</td>\n",
       "      <td>0.25</td>\n",
       "      <td>351.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tau  gamma  learning_rate  memory_size  replay_size  update_frequency  \\\n",
       "0  0.001  0.995         0.0005       200000          200                 5   \n",
       "1  0.001  0.990         0.0005       150000          200                 5   \n",
       "2  0.001  0.990         0.0005       200000          200                 5   \n",
       "3  0.001  0.990         0.0005       150000          200                 5   \n",
       "4  0.001  0.990         0.0005       150000          200                 5   \n",
       "5  0.001  0.995         0.0005       200000          200                 5   \n",
       "6  0.001  0.990         0.0005       200000          200                 5   \n",
       "7  0.001  0.995         0.0005       150000          200                 5   \n",
       "8  0.001  0.990         0.0008       100000          200                 5   \n",
       "9  0.001  0.990         0.0005       200000          200                 5   \n",
       "\n",
       "  hidden_layers  drop_out  solved_episodes  environmentSolved  \n",
       "0    [148, 148]      0.25            318.0               True  \n",
       "1    [100, 100]      0.25            324.0               True  \n",
       "2    [148, 148]      0.25            325.0               True  \n",
       "3    [148, 148]      0.25            330.0               True  \n",
       "4    [148, 148]      0.15            336.0               True  \n",
       "5    [100, 100]      0.25            343.0               True  \n",
       "6    [148, 148]      0.15            346.0               True  \n",
       "7    [148, 148]      0.25            347.0               True  \n",
       "8      [74, 74]      0.25            351.0               True  \n",
       "9    [100, 100]      0.25            351.0               True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TuningDf_loaded = pd.read_csv('ParameterResults.csv')\n",
    "TuningDf_loaded = TuningDf_loaded[(TuningDf_loaded['environmentSolved']==True)&(TuningDf_loaded['solved_episodes']<400)]\n",
    "TuningDf_loaded = TuningDf_loaded[[\"tau\",\"gamma\",\"learning_rate\",\"memory_size\",\"replay_size\",\"update_frequency\",\n",
    "                    \"hidden_layers\",\"drop_out\",\"solved_episodes\",\"environmentSolved\"]]\n",
    "TuningDf_loaded[:10] # Top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45249e26-5c65-4bb6-a71b-7436445eb7bc",
   "metadata": {},
   "source": [
    "## Training the final Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37501c9f-d79e-4195-b0e0-7da5fd678da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode              50              \tAverage Score: 0.70\t Mean Loss (MSE): 0.0049746\n",
      "Episode             100              \tAverage Score: 1.54\t Mean Loss (MSE): 0.005880\n",
      "Episode             150              \tAverage Score: 3.34\t Mean Loss (MSE): 0.008416\n",
      "Episode             200              \tAverage Score: 5.68\t Mean Loss (MSE): 0.012571\n",
      "Episode             250              \tAverage Score: 7.68\t Mean Loss (MSE): 0.017190\n",
      "Episode             300              \tAverage Score: 9.76\t Mean Loss (MSE): 0.021137\n",
      "Episode             350              \tAverage Score: 12.15\t Mean Loss (MSE): 0.024563\n",
      "Episode             400              \tAverage Score: 14.03\t Mean Loss (MSE): 0.027503\n",
      "Episode             418              \tAverage Score: 14.59\t Mean Loss (MSE): 0.028331"
     ]
    }
   ],
   "source": [
    "dqn(episodes=1800, tau = 0.001, gamma = 0.995, learning_rate = 0.0005, memory_size = 200000, replay_size = 200, \n",
    "    update_frequency = 5, hidden_layers = [148, 148], drop_out= 0.25, target_score = 15, \n",
    "    train_mode=True, parameter_tunning = False, save_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22549f11-86f7-437b-bc22-412697254ba8",
   "metadata": {},
   "source": [
    "# Wacthing a trained netowrk\n",
    "\n",
    "```python\n",
    "dqn(train_mode=False)\n",
    "```\n",
    "\n",
    "![GIF of Trained Network](Images/BananaAnimation2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98a27f-75b8-461b-bc8c-5946d463e237",
   "metadata": {},
   "source": [
    "## Plot the score from several games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ebede3-56f7-4a6a-b0d9-96e575e5f85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_trained = [dqn(train_mode=False,return_score=True) for i in range(5)]\n",
    "plot_trained_score(scores_trained,save_plot==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942d657-d308-432e-a0a9-138d8ec6fad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
